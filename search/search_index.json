{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"LSLAutoBIDS  <p> Tools to convert LSL + friends automatically to BIDS, and upload it to a Dataverse </p>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>Get started with LSLAutoBIDS by installing the package and its dependencies.</p>"},{"location":"#about-the-package","title":"\ud83d\udd30 About the package","text":"<p>This package automates the conversion of EEG recordings (xdf files) to BIDS (Brain Imaging Data Structure) format, integrates Datalad and uploads the data to  Dataverse. <code>lslautobids</code> is an open-source package written in <code>python</code> and  available as a pip package. </p>"},{"location":"#overview","title":"\ud83d\uddd2 Overview","text":"<p>LSLAutoBIDS is a Python tool series designed to automate the following tasks sequentially: - Convert recorded XDF files to BIDS format - Integrate the EEG data with non-EEG data (e.g., behavioral, other) for the complete dataset  - Datalad integration for version control for the integrated dataset - Upload the dataset to Dataverse  - Provide a command-line interface for cloning, configuring, and running the conversion process</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Automatic XDF to BIDS conversion</li> <li>DataLad integration for version control</li> <li>Dataverse integration for data sharing</li> <li>Configurable project management</li> <li>Support for behavioral data (non eeg files) in addition to EEG data</li> <li>Comprehensive logging and validation for BIDS compliance</li> </ul> <p>For more details, refer to the paper - Automating Data Integration and Publishing for Neuroimaging via LSLAutoBIDS.</p>"},{"location":"bids_convert_and_upload/","title":"BIDS Conversion","text":""},{"location":"bids_convert_and_upload/#bids-conversion-and-upload-pipeline-convert_to_bids_and_uploadpy","title":"BIDS Conversion and Upload Pipeline \u2699\ufe0f (<code>convert_to_bids_and_upload.py</code>)","text":"<p>The pipeline is designed to ensure:</p> <ol> <li> <p>Source files (EEG only) are preserved in a BIDS-compatible structure.</p> </li> <li> <p>EEG recordings are converted to BIDS format using MNE and validated against the BIDS standard.</p> </li> <li> <p>Behavioral and experimental metadata (also called other files in general in context on this project) are included and checked against project expectations.</p> </li> <li> <p>Project metadata is populated (dataset_description.json). This is required as a part of BIDS standard.</p> </li> <li> <p>The dataset is registered in Dataverse and optionally pushed/uploaded automatically.</p> </li> </ol>"},{"location":"bids_convert_and_upload/#entry-point-bids_process_and_upload","title":"Entry Point (<code>bids_process_and_upload()</code>)","text":"<ul> <li> <p>Reads project configuration (_config.toml) to check if a other computer (non eeg files) was used. (otherFilesUsed: true) <li> <p>Iterates over each processed file and extracts identifiers. For example, for a file named <code>sub-001_ses-001_task-Default_run-001_eeg.xdf</code>, it extracts:</p> <ul> <li> <p>Subject ID (sub-XXX) - 001 (<code>str</code> is accepted)</p> </li> <li> <p>Session ID (ses-YYY) - 001 (<code>str</code> is accepted)</p> </li> <li> <p>Run number (run-ZZZ) - 001 (<code>int</code> is accepted)</p> </li> <li> <p>Task name (task-Name) - Default (<code>str</code> is accepted)</p> </li> </ul> </li> <li> <p>Constructs the absolute path to the .xdf file from the project root.</p> </li> <li> <p>Calls BIDS.convert_to_bids() to handle conversion and validation.</p> </li> <li> <p>After all files are processed:</p> </li> <li> <p>Populates dataset_description.json.</p> <ul> <li> <p>Generates JSON metadata (<code>generate_json_file.py</code>).</p> </li> <li> <p>Creates/links a Dataverse dataset (<code>create_dataverse.py</code>).</p> </li> <li> <p>Initializes a DataLad dataset and links with Dataverse (<code>create_and_add_files_to_datalad_dataset.py</code>, <code>add_sibling_dataverse_in_folder.py</code>).</p> </li> <li> <p>Pushes data to Dataverse automatically (--yes) or via user confirmation.</p> </li> </ul> </li>"},{"location":"bids_convert_and_upload/#convert-to-bids-convert_to_bids","title":"Convert to BIDS (<code>convert_to_bids()</code>)","text":"<p>This function handles the core conversion of a XDF files to BIDS format and constructs the dataset structure. It performs the following steps:</p> <ol> <li> <p>Copy raw/behavioral/experiment files via <code>copy_source_files_to_bids()</code> (See section).</p> </li> <li> <p>Build a <code>BIDSPath</code> for the EEG recording:</p> <ul> <li> <p>Subject, session, run, task extracted from filename.</p> </li> <li> <p>File format: EEGLAB (.set) chosen to avoid BrainVision memory issues.</p> </li> </ul> </li> <li> <p>The function checks if the BIDS file already exists:</p> <ul> <li> <p>If it does, it logs and doesn\"t reconvert.</p> </li> <li> <p>If --redo_bids_conversion is specified, it overwrites existing files. This flag is passed from the command line while running <code>lslautobids run</code>.</p> </li> </ul> </li> <li> <p>If the file doesn't exist or is to be overwritten, it proceeds with conversion:</p> <ul> <li> <p>Load <code>.xdf</code> with <code>create_raw_xdf()</code>. (See section).</p> </li> <li> <p>Apply anonymization (daysback_min + anonymizationNumber from project TOML config).</p> </li> <li> <p>Write EEG data into BIDS folder via <code>write_raw_bids().</code></p> </li> </ul> </li> <li> <p>Validate results with <code>validate_bids()</code>. The function raises an error if validation fails and returns 0; otherwise returns 1 for success. (See section).</p> </li> <li> <p>The convertt_to_bids() function returns a status code indicating the result of the conversion:</p> <ul> <li> <p>1: Successful BIDS conversion and validation</p> </li> <li> <p>2: BIDS conversion already done i.e. file already existed, skipped conversion and validation</p> </li> <li> <p>0: BIDS Conversion done but validation failure</p> </li> </ul> </li> </ol>"},{"location":"bids_convert_and_upload/#copy-source-files-copy_source_files_to_bids","title":"Copy Source Files (<code>copy_source_files_to_bids()</code>)","text":"<p>This function ensures that the original source files (EEG and other/behavioral files) are also a part our dataset. These files can't be directly converted to BIDS format but we give the user the option to include them in the BIDS directory structure in a pseudo-BIDS format for completeness.</p> <ul> <li> <p>Copies the .xdf into the following structure:  <code>&lt;BIDS_ROOT&gt;/sourcedata/sub-XXX/ses-YYY/sub-XXX_ses-YYY_task-Name_run-ZZZ_eeg.xdf</code></p> </li> <li> <p>Adds <code>_raw</code> suffix to distinguish original files.</p> </li> <li> <p>If a file already exists, logs a message and skips copying.</p> </li> </ul> <p>If otherFilesUsed=True in project config file:</p> <ol> <li> <p>Behavioral files are copied via <code>_copy_behavioral_files()</code>.</p> <ul> <li>Validates required files against TOML config (<code>OtherFilesInfo</code>). In this config we add the the extensions of the expected other files. For example, in our testproject we use EyeList 1000 Plus eye tracker which generates .edf and .csv files. So we add these extensions as required other files. We also typically have mandatory labnotebook and participant info files in .tsv format.</li> <li>The <code>\"*.src\"=\"beh/{prefix}_target\"</code> allows users to easily add BIDS-compatible custom data from the experiments. Note that <code>json</code> sidecars are not automatically generated yet.</li> </ul> </li> <li> <p>Experimental files are copied via <code>_copy_experiment_files().</code></p> <ul> <li>Gathers files from the <code>&lt;PROJECTS_OTHER&gt;/experiment/</code> folder.</li> <li>Copies into BIDS <code>misc/</code> directory i.e. <code>&lt;BIDS_ROOT&gt;/misc/</code></li> <li>Compresses into <code>experiment.tar.gz</code>.</li> <li>Removes the uncompressed folder.</li> </ul> </li> </ol> <p>There is a flag in the <code>lslautobids run</code> command called <code>--redo_other_pc</code> which when specified, forces overwriting of existing other and experiment files in the BIDS dataset. This is useful if there are updates or corrections to the other/behavioral data that need to be reflected in the BIDS dataset.</p>"},{"location":"bids_convert_and_upload/#create-raw-xdf-create_raw_xdf","title":"Create Raw XDF (<code>create_raw_xdf()</code>)","text":"<p>This function reads the XDF file and creates an MNE Raw object. It performs the following steps: - Select EEG stream using match_streaminfos(type=\"EEG\").</p> <ul> <li> <p>Resample to the highest nominal sampling rate across streams (fs_new).</p> </li> <li> <p>Read .xdf with <code>read_raw_xdf()</code>, enabling interpolation and marker prefixing.</p> </li> <li> <p>Annotate missing values (annotate_nan) and clean invalid annotations (negative onset).</p> </li> <li> <p>Map known channel labels to MNE channel types (e.g., heog \u2192 eog, bipoc \u2192 misc). This is done using a predefined dictionary of channel mappings for our lab setup. This can be extended in future versions to include user-defined mappings.</p> </li> </ul> <p>This produces a clean, memory-efficient Raw object ready for BIDS conversion.</p>"},{"location":"bids_convert_and_upload/#bids-validation-validate_bids","title":"BIDS Validation (<code>validate_bids()</code>)","text":"<p>This function validates the generated BIDS files using the <code>bids-validator</code> package. It performs the following steps: - Walks through the BIDS directory. - Skips irrelevant files already ignored in <code>.bidsignore</code> (<code>misc</code> folder, some hidden files) - Uses <code>BIDSValidator</code> to validate relative paths.  - If any file fails validation, logs an error and returns 0 ; Otherwise, logs success and returns 1.</p>"},{"location":"bids_convert_and_upload/#populate-dataset_descriptionjson-populate_dataset_description_json","title":"Populate dataset_description.json (<code>populate_dataset_description_json()</code>)","text":"<p>This function generates the <code>dataset_description.json</code> file required by the BIDS standard. It performs the following steps: - Gathers metadata from the project configuration file (title, authors, license, etc.) from the project TOML config file. - Calls make_dataset_description() from mne_bids. - Overwrites existing file with updated values.</p>"},{"location":"bids_convert_and_upload/#datalad-and-dataverse-integration","title":"Datalad and Dataverse Integration","text":"<p>This part of the pipeline manages version control and data sharing using DataLad and Dataverse. After conversion, the following steps occur:</p> <ul> <li><code>generate_json_file()</code> \u2192 Generates supplementary metadata JSONs.</li> <li><code>create_dataverse()</code> \u2192 Creates a new dataset in Dataverse. Returns DOI + status.</li> <li><code>create_and_add_files_to_datalad_dataset()</code> \u2192 Initializes DataLad repo, adds files.</li> <li><code>add_sibling_dataverse_in_folder()</code> \u2192 Links DataLad dataset to Dataverse (if new dataset).</li> <li>push_files_to_dataverse() uploads files to Dataverse. It automatically uploads if --yes is set (This flag is set in <code>lslautobids run</code>), otherwise the function prompts user (y/n).</li> </ul>"},{"location":"cli/","title":"CLI Module","text":""},{"location":"cli/#cli-module-clipy","title":"CLI Module (<code>cli.py</code>)","text":"<p>The <code>lslautobids</code> command-line interface provides the main entry point for the application:</p> <ul> <li>Commands: <code>gen-proj-config</code>, <code>run</code>, <code>gen-dv-config</code>, <code>help</code></li> <li>Module mapping: Maps commands to their respective modules</li> <li>Argument handling: Processes and forwards command-line arguments</li> </ul>"},{"location":"cli/#key-points","title":"Key Points","text":"<ol> <li><code>lslautobids gen-proj-config</code> and <code>lslautobids gen-dv-config</code> commands generate configuration files for the project and Dataverse, respectively. This allows users to set up their project and Dataverse connection details easily before running the conversion and upload process</li> <li>The <code>lslautobids run</code> command executes the main conversion and upload process, using the configurations generated earlier. This command runs the entire pipeline from reading XDF files, converting them to BIDS format, integrating with DataLad, and uploading to Dataverse.</li> <li>The <code>lslautobids help</code> command provides usage information for the CLI, listing available commands and their descriptions.</li> </ol>"},{"location":"configuration/","title":"Configuration System","text":""},{"location":"configuration/#configuration-system","title":"Configuration System","text":"<p>The configuration system manages dataversse and project-specific settings using YAML and TOML files.</p>"},{"location":"configuration/#dataverse-and-project-root-configuration-gen_dv_configpy","title":"Dataverse and Project Root Configuration (<code>gen_dv_config.py</code>)","text":"<p>This module generates a global configuration file for Dataverse and project root directories. This is a one-time setup per system.  This file is stored in <code>~/.config/lslautobids/autobids_config.yaml</code> and contains: - Paths for BIDS, projects, and project_other directories : This allows users to specify where their eeg data, behavioral data, and converted BIDS data are stored on their system. This paths should be relative to the home/users directory of your system and string format.</p> <ul> <li>Dataverse connection details: Base URL, API key, and parent dataverse name for uploading datasets. Base URL is the URL of the dataverse server (e.g. https://darus.uni-stuttgart.de), API key is your personal API token for authentication (found in your dataverse account settings), and parent dataverse name is the name of the dataverse under which datasets will be created (this can be found in the URL when you are in the dataverses page just after 'dataverse/'). For example, if the URL is <code>https://darus.uni-stuttgart.de/dataverse/simtech_pn7_computational_cognitive_science</code>, then the parent dataverse name is <code>simtech_pn7_computational_cognitive_science</code>.</li> </ul> <p>Commands and arguments</p> <p>The command to generate the dataverse configuration file is: <pre><code>lslautobids gen-dv-config\n</code></pre> Currently, the package doesn't allow you to have multiple dataverse configurations. This will be added in future versions and can be easily adapted</p>"},{"location":"configuration/#project-configuration-gen_project_configpy","title":"Project Configuration (<code>gen_project_config.py</code>)","text":"<p>This module generates a project-specific configuration file in TOML format. This file is stored in the <code>projects/&lt;PROJECT_NAME&gt;/&lt;PROJECT_NAME&gt;_config.toml</code> file and contains: - Project metadata: Title, description, license, and authors, etc.</p> <p>Commands and arguments</p> <p>The command to generate the project configuration file is: <pre><code>lslautobids gen-proj-config --project &lt;projectname&gt;  \n</code></pre> - <code>--project &lt;projectname&gt;</code>: Specifies the name of the project for which the configuration file is to be generated. This argument is required. - <code>--standalone_toml</code> : (Optional) If provided, the generated configuration file will be a standalone TOML file in the current directory, without being placed in the project directory. - <code>--custom_dv_config</code> : (Optional) Path to a custom YAML configuration file (dataverse and project root configuration) for Dataverse and project root directories. If not provided, the default path <code>~/.config/lslautobids/autobids_config.yaml</code> will be used. This is specified to allow flexibility in using different configurations for different projects or testing purposes.</p>"},{"location":"contributions/","title":"Contributing","text":""},{"location":"contributions/#contribution-guidelines","title":"Contribution Guidelines","text":"<p>First of all, thanks for the interest!</p> <p>We welcome all kinds of contribution, including, but not limited to code, documentation, examples, configuration, issue creating, etc.</p> <p>Be polite and respectful, and follow the code of conduct.</p>"},{"location":"contributions/#bug-reports-and-discussions","title":"Bug reports and discussions","text":"<p>If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.</p>"},{"location":"contributions/#working-on-an-issue","title":"Working on an issue","text":"<p>If you found an issue that interests you, comment on that issue what your plans are. If the solution to the issue is clear, you can immediately create a pull request (see below). Otherwise, say what your proposed solution is and wait for a discussion around it.</p> <p>Tip</p> <p>Feel free to ping us after a few days if there are no responses.</p> <p>If your solution involves code (or something that requires running the package locally), check the developer's documentation. Otherwise, you can use the GitHub interface directly to create your pull request.</p>"},{"location":"data_organization/","title":"How the data is organized","text":"<p>In this project, we are using a sample xdf file along with the corresponding other files to demonstrate how the data inside the <code>projectname</code> folder is organized. This data should be organized in a specific way:</p>"},{"location":"data_organization/#recommended-project-organization-structure","title":"Recommended Project Organization Structure","text":"<p>For convenience, we have provided a recommended project organization  structure for the root directories to organize the data better.</p> <p>[!IMPORTANT] The recommended directory structure is not self generated. The user needs to create the directories and store the recorded and others data in them before running the conversion.</p> <p>The dataset (both recorded and converted) is stored in the parent <code>data</code> directory. The <code>data</code> directory has three subdirectories under which the entire project is stored. The recommended directory structure is as follows: <pre><code>data\n\u251c\u2500\u2500 bids                  # Converted BIDS data\n  \u251c\u2500\u2500 projectname1\n  \u251c\u2500\u2500 projectname2                \n\u251c\u2500\u2500 project_other      # Experimental/Behavioral files\n  \u251c\u2500\u2500 projectname1\n  \u251c\u2500\u2500 projectname2          \n\u251c\u2500\u2500 projects \n  \u251c\u2500\u2500 projectname1        # Recorded Raw data\n  \u251c\u2500\u2500 projectname2 \n</code></pre></p> <p>Here <code>./data/projects/</code>, <code>./data/project_other/</code>, <code>./data/bids/</code> are the root project directories. Each of this root directories will have a project name directory inside it and each project directory will have a subdirectory for each subject. </p>"},{"location":"data_organization/#projects-folder","title":"Projects Folder","text":"<p>This folder contains the recorded raw files (<code>xdf files</code>). We save these files in a pseudo BIDS format. The folder structure is as follows:</p> <pre><code>    projectname/\n    \u2514\u2500\u2500 subject_id\n        \u2514\u2500\u2500 session_id\n            \u2514\u2500\u2500 datatype\n                \u2514\u2500\u2500 datafiles\n</code></pre> <ul> <li>projectname - any descriptive name for the project</li> <li>subject_id - <code>sub-&lt;PARTICIPANTS_LABEL&gt;</code> Eg: sub-001</li> <li>session_id - <code>ses-&lt;PARTICIPANTS_LABEL&gt;</code>. Eg: ses-001</li> <li>datatype - any of the following: eeg, beh, iee, meg, ieeg, mri, pet, other. Find more information about the datatypes here. For our project, we are using <code>eeg</code> as the datatype.</li> <li>datafiles - the data files for the corresponding datatype. Eg: <code>`sub-001_ses-001_task-Duration_run-001_eeg.xdf for our eeg data.</code></li> </ul> <p>Filename Convention for the raw data files : <code>sub-&lt;subjectlabel&gt;_ses-&lt;sessionlabel&gt;_task-&lt;tasklabel&gt;_run-&lt;runlabel&gt;_ieeg.&lt;extension&gt;</code> - subjectlabel - <code>001, 002, abc, XF, ...</code> - sessionlabel - <code>001, 002, 003, ...</code> - tasklabel - <code>duration, mscoco, ...</code> - runlabel - <code>001, 002, 003, ...</code> (need to be an integer)</p>"},{"location":"data_organization/#project-other-folder","title":"Project Other Folder","text":"<p>This folder contains the experimental and behavioral files which we also store in the dataverse. The folder structure has to be as follows:</p> <pre><code>    projectname/\n    \u2514\u2500\u2500 experiment\n        \u2514\u2500\u2500 experimental_files (Matlab code, opensesame files, etc)\n    \u2514\u2500\u2500 data\n        \u2514\u2500\u2500 subject_id\n            \u2514\u2500\u2500 session_id\n                \u2514\u2500\u2500 beh\n                    \u2514\u2500\u2500 behavioral_files((lab notebook, CSV, EDF file, etc))\n</code></pre> <p>It is possible to modify the <code>src=target</code> syntax to \"skip\" folders via <code>..</code> (maybe we should simply allow <code>{prefix}</code> in the src as well =&gt; not yet implemented) - projectname - any descriptive name for the project - experiment - contains the experimental files for the project. Eg: showOther.m, showOther.py - data - contains the behavioral files for the corresponding subject. Eg: experimentalParameters.csv, eyetrackingdata.edf, results.tsv. </p> <p>You can get the filename convention for the data files here.</p>"},{"location":"data_organization/#bids-folder","title":"BIDS Folder","text":"<p>This folder contains the converted BIDS data files and other files we want to version control using <code>Datalad</code>. Since we are storing the entire dataset in the dataverse, we also store the raw xdf files and the associated other/behavioral files in the dataverse. The folder structure is as follows: <pre><code>\u2514\u2500\u2500 bids\n  \u2514\u2500\u2500projectname/\n    \u2514\u2500\u2500 code\n        \u2514\u2500\u2500 log files\n\n    \u2514\u2500\u2500 sub-&lt;label-sub&gt;\n        \u2514\u2500\u2500 ses-&lt;label-ses&gt;\n            \u2514\u2500\u2500 datatype (eg: eeg)\n                \u2514\u2500\u2500 converted BIDS files\n                    \u251c\u2500\u2500 sub-&lt;label-sub&gt;_ses-&lt;label-ses&gt;_task-Duration_run-001_eeg.vhdr\n                    \u251c\u2500\u2500 sub-001_ses-001_task-Duration_run-001_eeg.vmrk\n                    \u251c\u2500\u2500 sub-001_ses-001_task-Duration_run-001_eeg.eeg\n                    .........\n            \u2514\u2500\u2500 misc (added to .bidsignore)\n                \u2514\u2500\u2500 experimental files (This needs to stored in zip format)\n                \u2514\u2500\u2500 labnotebook, subjectform etc. \n    \u2514\u2500\u2500 sourcedata\n        \u2514\u2500\u2500 raw xdf files\n    \u2514\u2500\u2500 dataset_description.json\n    \u2514\u2500\u2500 participants.tsv\n    \u2514\u2500\u2500 participants.json\n    \u2514\u2500\u2500 README.md\n</code></pre></p> <ul> <li>projectname - any descriptive name for the project</li> <li>label - <code>001, 002, 003, ...</code></li> <li>datatype - any of the following: eeg, beh, iee, meg, ieeg, mri, pet, other. Find more information about the datatypes here.</li> </ul>"},{"location":"datalad_integration/","title":"Datalad Integration","text":""},{"location":"datalad_integration/#datalad-integration-datalad_createpy","title":"DataLad Integration (<code>datalad_create.py</code>)","text":"<p>The DataLad integration module manages the creation and updating of DataLad datasets for version control of the BIDS dataset. This function initializes a DataLad dataset at a given project path. The function performs the following steps: 1. Initialize commit message. It first sets a initialization message : \"LSL Auto BIDS: new files found and added\". 2. If flag == 0 (new dataset), the message is overwritten with \"LSL Auto BIDS: new datalad dataset created\". 3. Further, if flag == 0 we try to create a new DataLad dataset in the specified path (dataset_path) using <code>dl.create()</code>. This is similar to running <code>datalad create</code> from the command line for datalad CLI or <code>git init</code> for git CLI. This function is executed using :</p> <p><code>dl.create(dataset_path, force=True)</code></p> <p>Here, since the directory may already contain files (BIDS directory in our case), we use force=True to allow creation of the dataset even if files already exist and it is not an empty directory.</p> <ol> <li> <p>If it raises an exception (e.g., if the path is not a directory or if DataLad already exists ), it logs an error and exits the program.</p> </li> <li> <p>This function also changes the current working directory to the dataset_path using <code>os.chdir(dataset_path)</code> so subsequent DataLad operations (like <code>datalad save</code>, <code>datalad push</code>) run in the context of this dataset.</p> </li> <li> <p>This function also manages the .gitattributes file to ensure that large files are handled by git-annex and small text/metadata files are handled by git. This is done by appending specific patterns to the .gitattributes file. For example: <code>*.csv annex.largefiles=nothing</code> means .csv files are treated as small files and stored directly in git. Similarly, *    annex.largefiles=largerthan=100kb` means files larger than 100kb are managed by git-annex.</p> </li> <li> <p>Finally, it saves the changes to the DataLad dataset using <code>dl.save()</code> with the appropriate commit message. This save command stages all changes (new files, modified files) and commits them to the DataLad dataset.</p> </li> <li> <p>If flag!=0 (existing dataset), it skips the initialization step and directly saves any new changes to the existing DataLad dataset.</p> </li> </ol>"},{"location":"dataverse_integration/","title":"Dataverse Integration","text":""},{"location":"dataverse_integration/#dataverse-integration","title":"Dataverse Integration","text":""},{"location":"dataverse_integration/#1-generating-dataset-json-metadata-generate_dataset_jsonpy","title":"1. Generating dataset JSON Metadata (<code>generate_dataset_json.py</code>)","text":"<p>This module generates the <code>dataset.json</code> file required for creating a dataset in Dataverse. The function performs the following steps:</p> <ol> <li>Gathers metadata from the project configuration file (_config.toml) such as title, authors, description, license, etc. <li>Constructs a JSON structure that conforms to the Dataverse dataset metadata schema. This includes fields like title, author list, description, keywords, license, etc.</li> <li>Writes the constructed JSON to a file named <code>dataset.json</code> in the project directory. This file is then used when creating the dataset in Dataverse.</li>"},{"location":"dataverse_integration/#2-dataverse-dataset-creation-dataverse_dataset_createpy","title":"2. Dataverse Dataset Creation (<code>dataverse_dataset_create.py</code>)","text":"<p>This module handles the creation of a new dataset in Dataverse using the <code>pyDataverse</code> library. The function performs the following steps:</p> <ol> <li>Initialize <code>dataset.json</code> file path and read the JSON content. (See section)</li> <li>Sets up a Dataverse API connection using the base URL and API key from the global configuration file (<code>autobids_config.yaml</code>). This dataset then loads the  <code>dataset.json</code> into the Dataset. This json metadata populates the dataset metadata in Dataverse (title, authors, description, etc.), where we will eventually upload our datalad compatible BIDS dataset.</li> <li>The dataset JSON is validated using <code>ds.validate_json()</code>. If the validation passes only then we proceed to create the dataset in Dataverse using <code>dv.create_dataset()</code>.</li> <li>The function also checks if that dataset already exists in Dataverse (based on title) to avoid duplicates. For example, one dataverse dataset can contain data from multiple participants/subjects and we usually create a single dataset for the entire project but run the conversion for each subject separately. So we check if a dataset with the same title already exists in Dataverse.<ul> <li>Get all the datasets (pids) in the specified parent dataverse using <code>api.get_dataverse(parent_dataverse_name)</code>.</li> <li>Check if that the PID specified in the response matches the Dataverse PID specified in the project config file. If it does, we log a message and skip creation.</li> </ul> </li> <li>If no existing dataset is found, we create a new dataset using <code>api.create_dataset(parent_dataverse_name, ds.json())</code>. We then populate the returned dataset ID and DOI in the project configuration file (_config.toml) for using in future runs. <li>This function returns the dataset DOI and status code ( 1= dataverse dataset exists, 0= new dataset created)</li>"},{"location":"dataverse_integration/#3-linking-datalad-to-dataverse-link_datalad_dataversepy","title":"3. Linking DataLad to Dataverse (<code>link_datalad_dataverse.py</code>)","text":"<p>This module links the local DataLad dataset to the remote Dataverse dataset as a sibling. The function performs the following steps: 1. It first checks if the Dataverse is already created in the previous runs or it is just created in the current run (flag==0). If flag==0, it proceeds to link the DataLad dataset to Dataverse. 2. It runs the command <code>datalad add-sibling-dataverse dataverse_base_url doi_id</code>. This command adds the Dataverse as a sibling to the local DataLad dataset, allowing for synchronization and data management between the two. For lslautobids, we currently only allow to deposit data to Dataverse. In future version, we shall also add user controlled options for adding other siblings like github, gitlab, OpenNeuro, AWS etc.</p> <p>We chose Dataverse as it serves as both a repository and a data sharing platform, making it suitable for our needs. It also integrates well with DataLad and allows sharing datasets with collaborators or the public.</p> <p>Dataverse also provides features like versioning, but only after we publish the dataset. In our case, we keep the dataset in draft mode until we are ready to publish it (i.e. until all the participants/subjects data is uploaded). So we use DataLad for version control during the development and conversion phase to assure complete provenance of the dataset.</p>"},{"location":"dataverse_integration/#4-upload-to-dataverse-upload_to_dataversepy","title":"4. Upload to Dataverse (<code>upload_to_dataverse.py</code>)","text":"<p>This module handles the uploading of files from the local DataLad dataset to the remote Dataverse dataset. The function performs the following steps: 1. It runs the command <code>datalad push --to dataverse</code> to push the files from the local DataLad dataset to the linked Dataverse dataset. This command uploads all changes (new files, modified files) to Dataverse. 2. If the <code>--yes</code> flag is set (in <code>lslautobids run</code>), it automatically pushes the files without user confirmation. Otherwise, it prompts the user for confirmation before proceeding with the upload.</p>"},{"location":"developers_documentation/","title":"LSLAutoBIDS Developer's Documentation","text":""},{"location":"developers_documentation/#overview","title":"Overview","text":"<p>LSLAutoBIDS is a Python tool series designed to automate the following tasks sequentially: - Convert recorded XDF files to BIDS format - Integrate the EEG data with non-EEG data (e.g., behavioral, other) for the complete dataset  - Datalad integration for version control for the integrated dataset - Upload the dataset to Dataverse  - Provide a command-line interface for cloning, configuring, and running the conversion process</p>"},{"location":"developers_documentation/#key-features","title":"Key Features","text":"<ul> <li>Automatic XDF to BIDS conversion</li> <li>DataLad integration for version control</li> <li>Dataverse integration for data sharing</li> <li>Configurable project management</li> <li>Support for behavioral data (non eeg files) in addition to EEG data</li> <li>Comprehensive logging and validation for BIDS compliance</li> </ul>"},{"location":"developers_documentation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Core Components</li> <li>1. CLI Module (<code>cli.py</code>)</li> <li>2. Configuration System<ul> <li>1. Dataverse and Project Root Configuration (<code>gen_dv_config.py</code>)</li> <li>2. Project Configuration (<code>gen_project_config.py</code>)</li> </ul> </li> <li> <p>3. File Processing Pipeline (<code>processing_new_files.py</code>)</p> <ul> <li>1. Detection of New Files (<code>check_for_new_data()</code>)</li> <li>2. Filtering (<code>check_for_new_files()</code>)</li> <li>3. Validation and Preparation \u2013 (<code>process_new_files()</code>)</li> <li>4. User Prompt &amp; Conversion</li> <li>5. BIDS Conversion and Upload (<code>bids_process_and_upload()</code>)</li> <li>6. Supporting Utility</li> </ul> </li> <li> <p>4. BIDS Conversion and Upload Pipeline \u2699\ufe0f (<code>convert_to_bids_and_upload.py</code>)</p> <ul> <li>1. Entry Point (<code>bids_process_and_upload()</code>)</li> <li>2. Convert to BIDS (<code>convert_to_bids()</code>)</li> <li>3. Copy Source Files (<code>copy_source_files_to_bids()</code>)</li> <li>4. Create Raw XDF (<code>create_raw_xdf()</code>)</li> <li>5. BIDS Validation (<code>validate_bids()</code>)</li> <li>6. Populate dataset_description.json (<code>populate_dataset_description_json()</code>)</li> <li>7. Datalad and Dataverse Integration</li> </ul> </li> <li>5. DataLad Integration (<code>datalad_create.py</code>)</li> <li>6. Dataverse Integration<ul> <li>1. Dataverse Dataset Creation (<code>dataverse_dataset_create.py</code>)</li> <li>2. Linking DataLad to Dataverse (<code>link_datalad_dataverse.py</code>)</li> <li>3. Generating dataset JSON Metadata (<code>generate_dataset_json.py</code>)</li> <li>4. Upload to Dataverse (<code>upload_to_dataverse.py</code>)</li> </ul> </li> <li> <p>Other Utility Modules</p> <ul> <li>1. Global Configuration Management (<code>config_globals.py</code>)</li> <li>2. Logging Configuration (<code>config_logger.py</code>)</li> <li>3. Utility Functions (<code>utils.py</code>)</li> </ul> </li> <li> <p>Testing</p> </li> <li>Running Tests</li> </ul>"},{"location":"developers_documentation/#core-components","title":"Core Components","text":""},{"location":"developers_documentation/#1-cli-module-clipy","title":"1. CLI Module (<code>cli.py</code>)","text":"<p>The command-line interface provides the main entry point for the application:</p> <ul> <li>Commands: <code>gen-proj-config</code>, <code>run</code>, <code>gen-dv-config</code>, <code>help</code></li> <li>Module mapping: Maps commands to their respective modules</li> <li>Argument handling: Processes and forwards command-line arguments</li> </ul>"},{"location":"developers_documentation/#key-points","title":"Key Points","text":"<ol> <li><code>lslautobids en-proj-config</code> and <code>lslautobids gen-dv-config</code> commands generate configuration files for the project and Dataverse, respectively. This allows users to set up their project and Dataverse connection details easily before running the conversion and upload process</li> <li>The <code>lslautobids run</code> command executes the main conversion and upload process, using the configurations generated earlier. This command runs the entire pipeline from reading XDF files, converting them to BIDS format, integrating with DataLad, and uploading to Dataverse.</li> <li>The <code>lslautobids help</code> command provides usage information for the CLI, listing available commands and their descriptions.</li> </ol>"},{"location":"developers_documentation/#2-configuration-system","title":"2. Configuration System","text":"<p>The configuration system manages dataversse and project-specific settings using YAML and TOML files.</p>"},{"location":"developers_documentation/#1-dataverse-and-project-root-configuration-gen_dv_configpy","title":"1. Dataverse and Project Root Configuration (<code>gen_dv_config.py</code>)","text":"<p>This module generates a global configuration file for Dataverse and project root directories. This is a one-time setup per system.  This file is stored in <code>~/.config/lslautobids/autobids_config.yaml</code> and contains: - Paths for BIDS, projects, and project_other directories : This allows users to specify where their eeg data, behavioral data, and converted BIDS data are stored on their system. This paths should be relative to the home/users directory of your system and string format.</p> <ul> <li>Dataverse connection details: Base URL, API key, and parent dataverse name for uploading datasets. Base URL is the URL of the dataverse server (e.g. https://darus.uni-stuttgart.de), API key is your personal API token for authentication (found in your dataverse account settings), and parent dataverse name is the name of the dataverse under which datasets will be created (this can be found in the URL when you are in the dataverses page just after 'dataverse/'). For example, if the URL is <code>https://darus.uni-stuttgart.de/dataverse/simtech_pn7_computational_cognitive_science</code>, then the parent dataverse name is <code>simtech_pn7_computational_cognitive_science</code>.</li> </ul> <p>Commands and arguments</p> <p>The command to generate the dataverse configuration file is: <pre><code>lslautobids gen-dv-config\n</code></pre> _Currently, the package doesn't allow you to have multiple dataverse configurations. This will be added in future versions.</p>"},{"location":"developers_documentation/#2-project-configuration-gen_project_configpy","title":"2. Project Configuration (<code>gen_project_config.py</code>)","text":"<p>This module generates a project-specific configuration file in TOML format. This file is stored in the <code>projects/&lt;PROJECT_NAME&gt;/&lt;PROJECT_NAME&gt;_config.toml</code> file and contains: - Project metadata: Title, description, license, and authors, etc.  <p>Commands and arguments</p> <p>The command to generate the project configuration file is: <pre><code>lslautobids gen-proj-config --project &lt;projectname&gt;  \n</code></pre> - <code>--project &lt;projectname&gt;</code>: Specifies the name of the project for which the configuration file is to be generated. This argument is required. - <code>--standalone_toml</code> : (Optional) If provided, the generated configuration file will be a standalone TOML file in the current directory, without being placed in the project directory. - <code>--custom_dv_config</code> : (Optional) Path to a custom YAML configuration file (dataverse and project root configuration) for Dataverse and project root directories. If not provided, the default path <code>~/.config/lslautobids/autobids_config.yaml</code> will be used. This is specified to allow flexibility in using different configurations for different projects or testing purposes.</p>"},{"location":"developers_documentation/#3-file-processing-pipeline-processing_new_filespy","title":"3. File Processing Pipeline (<code>processing_new_files.py</code>)","text":"<p>The file processing part of the pipeline handles finding and processing new XDF files in the specified project directory:</p> <p>The pipeline ensures that all newly added data files are:</p> <ol> <li> <p>Detected since the last run.</p> </li> <li> <p>Filtered based on ignored subjects and tasks (these are specified in the project configuration file).</p> </li> <li> <p>Validated against duplicate or malformed filenames.</p> </li> <li> <p>Registered in the project configuration file (e.g., tasks ,etc.).</p> </li> <li> <p>Converted to BIDS format and uploaded, based on user confirmation or auto-run flags.</p> </li> </ol>"},{"location":"developers_documentation/#1-detection-of-new-files-check_for_new_data","title":"1. Detection of New Files (<code>check_for_new_data()</code>)","text":"<ul> <li> <p>Entry point of the script (from <code>main.py</code>).</p> </li> <li> <p>Reads project configuration (_config.toml) to identify ignored subjects and excluded tasks (which we don't want to process and include in the BIDS dataset). <li> <p>Calls <code>check_for_new_files()</code> to scan the project directory for files modified after the last recorded run.</p> </li>"},{"location":"developers_documentation/#2-filtering-check_for_new_files","title":"2. Filtering (<code>check_for_new_files()</code>)","text":"<ul> <li> <p>Uses <code>last_run_log.txt</code> to determine which files are new since the last check.</p> </li> <li> <p>Excludes:</p> <ul> <li> <p>Ignored subjects (defined in the project TOML config).</p> </li> <li> <p>Ignored tasks (tasks explicitly excluded in the project TOML config).</p> </li> </ul> </li> <li> <p>Returns a list of candidate <code>.xdf</code> files to process or outputs a message if no new files are found and exits.</p> </li> </ul>"},{"location":"developers_documentation/#3-validation-and-preparation-process_new_files","title":"3. Validation and Preparation \u2013 (<code>process_new_files()</code>)","text":"<p>For each new file:</p> <ul> <li> <p>File type check: Only .xdf files are considered.</p> </li> <li> <p>Duplicate prevention: Files with a _old suffix (e.g., sub-001_ses-01_task-Default_old.xdf) are flagged as duplicates. The pipeline logs an error and halts execution to avoid accidental overwrites. This is currently a specific duplicate check for LSL recordings. We shall extend this in future versions.</p> </li> <li> <p>Task extraction: Task names are parsed from filenames (e.g., sub-888_ses-001_task-Default_run-001_eeg.xdf \u2192 task = Default).</p> </li> <li> <p>Configuration update: Newly discovered tasks are appended to the TOML config under [Tasks.tasks].</p> </li> </ul>"},{"location":"developers_documentation/#4-user-prompt-conversion","title":"4. User Prompt &amp; Conversion","text":"<ul> <li>If cli_args.yes is set (as a flag while running <code>lslautobids run</code>), the pipeline skips user interaction and proceeds directly to conversion. In general, this flag sets all the user prompts throughout the pipeline to 'yes', allowing for fully automated runs. </li> <li> <p>Otherwise, the user is prompted to confirm whether BIDS conversion should start.</p> </li> <li> <p>If declined, the process halts with a warning.</p> </li> </ul>"},{"location":"developers_documentation/#5-bids-conversion-and-upload-bids_process_and_upload","title":"5. BIDS Conversion and Upload (<code>bids_process_and_upload()</code>)","text":"<ul> <li> <p>Invoked only if the user confirms (or auto-run flag is set).</p> </li> <li> <p>Handles BIDS formatting and uploading of the processed files.</p> </li> </ul>"},{"location":"developers_documentation/#6-supporting-utility","title":"6. Supporting Utility","text":"<ul> <li><code>_clear_last_run_log()</code> :Clears the last run timestamp (last_run_log.txt) when --redo_bids_conversion is specified (as a flag in <code>lslautobids run</code>), forcing reprocessing of all files.</li> </ul>"},{"location":"developers_documentation/#4-bids-conversion-and-upload-pipeline-convert_to_bids_and_uploadpy","title":"4. BIDS Conversion and Upload Pipeline \u2699\ufe0f (<code>convert_to_bids_and_upload.py</code>)","text":"<p>The pipeline is designed to ensure:</p> <ol> <li> <p>Source files (EEG only) are preserved in a BIDS-compatible structure.</p> </li> <li> <p>EEG recordings are converted to BIDS format using MNE and validated against the BIDS standard.</p> </li> <li> <p>Behavioral and experimental metadata (also called other files in general in context on this project) are included and checked against project expectations.</p> </li> <li> <p>Project metadata is populated (dataset_description.json). This is required as a part of BIDS standard.</p> </li> <li> <p>The dataset is registered in Dataverse and optionally pushed/uploaded automatically.</p> </li> </ol>"},{"location":"developers_documentation/#1-entry-point-bids_process_and_upload","title":"1. Entry Point (<code>bids_process_and_upload()</code>)","text":"<ul> <li> <p>Reads project configuration (_config.toml) to check if a other computer (non eeg files) was used. (otherFilesUsed: true) <li> <p>Iterates over each processed file and extracts identifiers. For example, for a file named <code>sub-001_ses-001_task-Default_run-001_eeg.xdf</code>, it extracts:</p> <ul> <li> <p>Subject ID (sub-XXX) - 001 (<code>str</code> is accepted)</p> </li> <li> <p>Session ID (ses-YYY) - 001 (<code>str</code> is accepted)</p> </li> <li> <p>Run number (run-ZZZ) - 001 (<code>int</code> is accepted)</p> </li> <li> <p>Task name (task-Name) - Default (<code>str</code> is accepted)</p> </li> </ul> </li> <li> <p>Constructs the absolute path to the .xdf file from the project root.</p> </li> <li> <p>Calls BIDS.convert_to_bids() to handle conversion and validation.</p> </li> <li> <p>After all files are processed:</p> </li> <li> <p>Populates dataset_description.json.</p> <ul> <li> <p>Generates JSON metadata (<code>generate_json_file.py</code>).</p> </li> <li> <p>Creates/links a Dataverse dataset (<code>create_dataverse.py</code>).</p> </li> <li> <p>Initializes a DataLad dataset and links with Dataverse (<code>create_and_add_files_to_datalad_dataset.py</code>, <code>add_sibling_dataverse_in_folder.py</code>).</p> </li> <li> <p>Pushes data to Dataverse automatically (--yes) or via user confirmation.</p> </li> </ul> </li>"},{"location":"developers_documentation/#2-convert-to-bids-convert_to_bids","title":"2. Convert to BIDS (<code>convert_to_bids()</code>)","text":"<p>This function handles the core conversion of a XDF files to BIDS format and constructs the dataset structure. It performs the following steps:</p> <ol> <li> <p>Copy raw/behavioral/experiment files via <code>copy_source_files_to_bids()</code> (See section).</p> </li> <li> <p>Build a <code>BIDSPath</code> for the EEG recording:</p> <ul> <li> <p>Subject, session, run, task extracted from filename.</p> </li> <li> <p>File format: EEGLAB (.set) chosen to avoid BrainVision memory issues.</p> </li> </ul> </li> <li> <p>The function checks if the BIDS file already exists:</p> <ul> <li> <p>If it does, it logs and doesn\"t reconvert.</p> </li> <li> <p>If --redo_bids_conversion is specified, it overwrites existing files. This flag is passed from the command line while running <code>lslautobids run</code>.</p> </li> </ul> </li> <li> <p>If the file doesn't exist or is to be overwritten, it proceeds with conversion:</p> <ul> <li> <p>Load <code>.xdf</code> with <code>create_raw_xdf()</code>. (See section).</p> </li> <li> <p>Apply anonymization (daysback_min + anonymizationNumber from project TOML config).</p> </li> <li> <p>Write EEG data into BIDS folder via <code>write_raw_bids().</code></p> </li> </ul> </li> <li> <p>Validate results with <code>validate_bids()</code>. The function raises an error if validation fails and returns 0; otherwise returns 1 for success. (See section).</p> </li> <li> <p>The convertt_to_bids() function returns a status code indicating the result of the conversion:</p> <ul> <li> <p>1: Successful BIDS conversion and validation</p> </li> <li> <p>2: BIDS conversion already done i.e. file already existed, skipped conversion and validation</p> </li> <li> <p>0: BIDS Conversion done but validation failure</p> </li> </ul> </li> </ol>"},{"location":"developers_documentation/#3-copy-source-files-copy_source_files_to_bids","title":"3. Copy Source Files (<code>copy_source_files_to_bids()</code>)","text":"<p>This function ensures that the original source files (EEG and other/behavioral files) are also a part our dataset. These files can't be directly converted to BIDS format but we give the user the option to include them in the BIDS directory structure in a pseudo-BIDS format for completeness.</p> <ul> <li> <p>Copies the .xdf into the following structure:  <code>&lt;BIDS_ROOT&gt;/sourcedata/sub-XXX/ses-YYY/sub-XXX_ses-YYY_task-Name_run-ZZZ_eeg.xdf</code></p> </li> <li> <p>Adds <code>_raw</code> suffix to distinguish original files.</p> </li> <li> <p>If a file already exists, logs a message and skips copying.</p> </li> </ul> <p>If otherFilesUsed=True in project config file:</p> <ol> <li> <p>Behavioral files are copied via <code>_copy_behavioral_files()</code>.</p> <ul> <li>Validates required files against TOML config (<code>OtherFilesInfo</code>). In this config we add the the extensions of the expected other files. For example, in our testproject we use EyeList 1000 Plus eye tracker which generates .edf and .csv files. So we add these extensions as required other files. We also typically use a mandatory labnotebook and participant info files in .tsv format. Currently it is not possible to convert files in this step, but should maybe become possible for e.g. <code>EDF</code> files and <code>CSV=&gt;TSV</code> files</li> <li>follows the src=target regexp syntax to copy files over</li> </ul> </li> <li> <p>Experimental files are copied via <code>_copy_experiment_files().</code></p> <ul> <li>Gathers files from the experiment folder.</li> <li>Copies into BIDS <code>misc/</code> directory i.e. <code>&lt;BIDS_ROOT&gt;/misc/</code></li> <li>Compresses into experiment.tar.gz.</li> <li>Removes the uncompressed folder.</li> </ul> </li> </ol> <p>There is a flag in the <code>lslautobids run</code> command called <code>--redo_other_pc</code> which when specified, forces overwriting of existing other and experiment files in the BIDS dataset. This is useful if there are updates or corrections to the other/behavioral data that need to be reflected in the BIDS dataset.</p>"},{"location":"developers_documentation/#4-create-raw-xdf-create_raw_xdf","title":"4. Create Raw XDF (<code>create_raw_xdf()</code>)","text":"<p>This function reads the XDF file and creates an MNE Raw object. It performs the following steps: - Select EEG stream using match_streaminfos(type=\"EEG\").</p> <ul> <li> <p>Resample to the highest nominal sampling rate across streams (fs_new).</p> </li> <li> <p>Read .xdf with <code>read_raw_xdf()</code>, enabling interpolation and marker prefixing.</p> </li> <li> <p>Annotate missing values (annotate_nan) and clean invalid annotations (negative onset).</p> </li> <li> <p>Map known channel labels to MNE channel types (e.g., heog \u2192 eog, bipoc \u2192 misc). This is done using a predefined dictionary of channel mappings for our lab setup. This can be extended in future versions to include user-defined mappings.</p> </li> </ul> <p>This produces a clean, memory-efficient Raw object ready for BIDS conversion.</p>"},{"location":"developers_documentation/#5-bids-validation-validate_bids","title":"5. BIDS Validation (<code>validate_bids()</code>)","text":"<p>This function validates the generated BIDS files using the <code>bids-validator</code> package. It performs the following steps: - Walks through the BIDS directory. - Skips irrelevant files: (<code>misc</code>-folder, hidden/system files.) - Uses <code>BIDSValidator</code> to validate relative paths.  - If any file fails validation, logs an error and returns 0 ; Otherwise, logs success and returns 1.</p>"},{"location":"developers_documentation/#6-populate-dataset_descriptionjson-populate_dataset_description_json","title":"6. Populate dataset_description.json (<code>populate_dataset_description_json()</code>)","text":"<p>This function generates the <code>dataset_description.json</code> file required by the BIDS standard. It performs the following steps: - Gathers metadata from the project configuration file (title, authors, license, etc.) from the project TOML config file. - Calls make_dataset_description() from mne_bids. - Overwrites existing file with updated values.</p>"},{"location":"developers_documentation/#7-datalad-and-dataverse-integration","title":"7. Datalad and Dataverse Integration","text":"<p>This part of the pipeline manages version control and data sharing using DataLad and Dataverse. After conversion, the following steps occur:</p> <ul> <li><code>generate_json_file()</code> \u2192 Generates supplementary metadata JSONs.</li> <li><code>create_dataverse()</code> \u2192 Creates a new dataset in Dataverse. Returns DOI + status.</li> <li><code>create_and_add_files_to_datalad_dataset()</code> \u2192 Initializes DataLad repo, adds files.</li> <li><code>add_sibling_dataverse_in_folder()</code> \u2192 Links DataLad dataset to Dataverse (if new dataset).</li> <li>push_files_to_dataverse() uploads files to Dataverse. It automatically uploads if --yes is set (This flag is set in <code>lslautobids run</code>), otherwise the function prompts user (y/n).</li> </ul>"},{"location":"developers_documentation/#4-datalad-integration-datalad_createpy","title":"4. DataLad Integration (<code>datalad_create.py</code>)","text":"<p>The DataLad integration module manages the creation and updating of DataLad datasets for version control of the BIDS dataset. This function initializes a DataLad dataset at a given project path. The function performs the following steps: 1. Initialize commit message. It first sets a initialization message : \"LSL Auto BIDS: new files found and added\". 2. If flag == 0 (new dataset), the message is overwritten with \"LSL Auto BIDS: new datalad dataset created\". 3. Further, if flag == 0 we try to create a new DataLad dataset in the specified path (dataset_path) using <code>dl.create()</code>. This is similar to running <code>datalad create</code> from the command line for datalad CLI or <code>git init</code> for git CLI. This function is executed using :</p> <p><code>dl.create(dataset_path, force=True)</code></p> <p>Here, since the directory may already contain files (BIDS directory in our case), we use force=True to allow creation of the dataset even if files already exist and it is not an empty directory.</p> <ol> <li> <p>If it raises an exception (e.g., if the path is not a directory or if DataLad already exists ), it logs an error and exits the program.</p> </li> <li> <p>This function also changes the current working directory to the dataset_path using <code>os.chdir(dataset_path)</code> so subsequent DataLad operations (like <code>datalad save</code>, <code>datalad push</code>) run in the context of this dataset.</p> </li> <li> <p>This function also manages the .gitattributes file to ensure that large files are handled by git-annex and small text/metadata files are handled by git. This is done by appending specific patterns to the .gitattributes file. For example: <code>*.csv annex.largefiles=nothing</code> means .csv files are treated as small files and stored directly in git. Similarly, *    annex.largefiles=largerthan=100kb` means files larger than 100kb are managed by git-annex.</p> </li> <li> <p>Finally, it saves the changes to the DataLad dataset using <code>dl.save()</code> with the appropriate commit message. This save command stages all changes (new files, modified files) and commits them to the DataLad dataset.</p> </li> <li> <p>If flag!=0 (existing dataset), it skips the initialization step and directly saves any new changes to the existing DataLad dataset.</p> </li> </ol>"},{"location":"developers_documentation/#5-dataverse-integration","title":"5. Dataverse Integration","text":""},{"location":"developers_documentation/#1-dataverse-dataset-creation-dataverse_dataset_createpy","title":"1. Dataverse Dataset Creation (<code>dataverse_dataset_create.py</code>)","text":"<p>This module handles the creation of a new dataset in Dataverse using the <code>pyDataverse</code> library. The function performs the following steps:</p> <ol> <li>Initialize <code>dataset.json</code> file path and read the JSON content. (See section)</li> <li>Sets up a Dataverse API connection using the base URL and API key from the global configuration file (<code>autobids_config.yaml</code>). This dataset then loads the  <code>dataset.json</code> into the Dataset. This json metadata populates the dataset metadata in Dataverse (title, authors, description, etc.), where we will eventually upload our datalad compatible BIDS dataset.</li> <li>The dataset JSON is validated using <code>ds.validate_json()</code>. If the validation passes only then we proceed to create the dataset in Dataverse using <code>dv.create_dataset()</code>.</li> <li>The function also checks if that dataset already exists in Dataverse (based on title) to avoid duplicates. For example, one dataverse dataset can contain data from multiple participants/subjects and we usually create a single dataset for the entire project but run the conversion for each subject separately. So we check if a dataset with the same title already exists in Dataverse.<ul> <li>Get all the datasets (pids) in the specified parent dataverse using <code>api.get_dataverse(parent_dataverse_name)</code>.</li> <li>Check if that the PID specified in the response matches the Dataverse PID specified in the project config file. If it does, we log a message and skip creation.</li> </ul> </li> <li>If no existing dataset is found, we create a new dataset using <code>api.create_dataset(parent_dataverse_name, ds.json())</code>. We then populate the returned dataset ID and DOI in the project configuration file (_config.toml) for using in future runs. <li>This function returns the dataset DOI and status code ( 1= dataverse dataset exists, 0= new dataset created)</li>"},{"location":"developers_documentation/#2-linking-datalad-to-dataverse-link_datalad_dataversepy","title":"2. Linking DataLad to Dataverse (<code>link_datalad_dataverse.py</code>)","text":"<p>This module links the local DataLad dataset to the remote Dataverse dataset as a sibling. The function performs the following steps: 1. It first checks if the Dataverse is already created in the previous runs or it is just created in the current run (flag==0). If flag==0, it proceeds to link the DataLad dataset to Dataverse. 2. It runs the command <code>datalad add-sibling-dataverse dataverse_base_url doi_id</code>. This command adds the Dataverse as a sibling to the local DataLad dataset, allowing for synchronization and data management between the two. For lslautobids, we currently only allow to deposit data to Dataverse. In future version, we shall also add user controlled options for adding other siblings like github, gitlab, OpenNeuro, AWS etc.</p> <p>We chose Dataverse as it serves as both a repository and a data sharing platform, making it suitable for our needs. It also integrates well with DataLad and allows sharing datasets with collaborators or the public.</p> <p>Dataverse also provides features like versioning, but only after we publish the dataset. In our case, we keep the dataset in draft mode until we are ready to publish it (i.e. until all the participants/subjects data is uploaded). So we use DataLad for version control during the development and conversion phase to assure complete provenance of the dataset.</p>"},{"location":"developers_documentation/#4-generating-dataset-json-metadata-generate_dataset_jsonpy","title":"4 Generating dataset JSON Metadata (<code>generate_dataset_json.py</code>)","text":"<p>This module generates the <code>dataset.json</code> file required for creating a dataset in Dataverse. The function performs the following steps:</p> <ol> <li>Gathers metadata from the project configuration file (_config.toml) such as title, authors, description, license, etc. <li>Constructs a JSON structure that conforms to the Dataverse dataset metadata schema. This includes fields like title, author list, description, keywords, license, etc.</li> <li>Writes the constructed JSON to a file named <code>dataset.json</code> in the project directory. This file is then used when creating the dataset in Dataverse.</li>"},{"location":"developers_documentation/#5-upload-to-dataverse-upload_to_dataversepy","title":"5. Upload to Dataverse (<code>upload_to_dataverse.py</code>)","text":"<p>This module handles the uploading of files from the local DataLad dataset to the remote Dataverse dataset. The function performs the following steps: 1. It runs the command <code>datalad push --to dataverse</code> to push the files from the local DataLad dataset to the linked Dataverse dataset. This command uploads all changes (new files, modified files) to Dataverse. 2. If the <code>--yes</code> flag is set (in <code>lslautobids run</code>), it automatically pushes the files without user confirmation. Otherwise, it prompts the user for confirmation before proceeding with the upload.</p>"},{"location":"developers_documentation/#other-utility-modules","title":"Other Utility Modules","text":""},{"location":"developers_documentation/#1-global-configuration-management-config_globalspy","title":"1. Global Configuration Management (<code>config_globals.py</code>)","text":"<p>This module manages global configuration settings and command-line arguments using a singleton pattern. The <code>CLIArgs</code> class ensures that there is only one instance of the configuration throughout the application. It provides methods to parse and retrieve command-line arguments and global configuration settings (lslautobids_config.yaml), which are then used across various modules.</p>"},{"location":"developers_documentation/#2-logging-configuration-config_loggerpy","title":"2. Logging Configuration (<code>config_logger.py</code>)","text":"<p>This module sets up a global logger for the application. The <code>get_logger()</code> function creates and configures a logger instance with a specified project name. It ensures that all log messages are formatted consistently and that log levels are set appropriately. The final log file is stored in the <code>&lt;BIDS_ROOT&gt;/&lt;project-name&gt;/code/</code> folder of the BIDS dataset.</p> <p>The log file is also available as a part of the created dataset.</p>"},{"location":"developers_documentation/#3-utility-functions-utilspy","title":"3. Utility Functions (<code>utils.py</code>)","text":"<p>This module contains various utility functions used across the application.  1. <code>get_user_input</code> : Handles user prompts and input validation. This function allows five attempts for valid input before exiting. The function takes the user prompt message as an argument and returns the user input. 2. <code>read_toml_file</code> : Reads and parses a TOML file, returning its contents as a dictionary. 3. <code>write_toml_file</code> : Writes a dictionary to a TOML file.</p>"},{"location":"developers_documentation/#testing","title":"Testing","text":"<p>The testing framework uses <code>pytest</code> to validate the functionality of the core components.</p> <ul> <li> <p>The tests are located in the <code>tests/</code> directory and cover various modules including configuration generation, file processing, BIDS conversion, DataLad integration, and Dataverse interaction. (Work in progress)</p> </li> <li> <p>The test directory contains :</p> <ul> <li><code>test_utils</code> : Directory containing utility functions needed across multiple test files.</li> <li><code>testcases</code> : Directory containing all the tests in a in a directory structure - <code>test_&lt;test_name&gt;</code>.</li> <li>Each <code>test_&lt;test_name&gt;</code> directory contains a <code>data</code> folder with sample data for that test and a <code>test_&lt;test_name&gt;.py</code> file with the actual test cases.</li> <li><code>run_all_tests.py</code> : A script to run all the tests in the <code>testcases</code> directory sequentially.</li> </ul> </li> </ul> <p>Tests will be added continuously as new features are added and existing features are updated.</p>"},{"location":"developers_documentation/#running-tests","title":"Running Tests","text":"<p>To run the tests, navigate to the <code>tests/</code> directory and execute: <code>python tests/run_all_tests.py</code></p> <p>These tests ensure that each component functions as expected and that the overall pipeline works seamlessly. This tests will also be triggered automatically on each push or PR to the main repository using GitHub Actions.</p>"},{"location":"developers_documentation/#miscellianeous-points","title":"Miscellianeous Points","text":"<ul> <li>To the current date, only EEG data is supported for BIDS conversion. Support for other modalities like Eye-tracking, etc,. in the BIDS format is not yet supported. Hence, LSLAutoBIDS relies on semi-BIDS data structures for those data and use user-definable regular expressions to match expected data-files. A future planned feature is to provide users more flexibility, especially in naming / sorting non-standard files. Currently, the user can only specify the expected file extensions for other/behavioral data and is automatically renamed to include sub-XXX_ses-YYY_ prefix if missing and also copied to pseudo-BIDS folder structure like <code>&lt;BIDS_ROOT&gt;/sourcedata/sub-XXX/ses-YYY/</code>, <code>&lt;BIDS_ROOT&gt;/misc/experiment.tar.gz</code> etc,.</li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>These are some frequently asked questions regarding the LSLAutoBIDS tool and workflow.</p> <p>1. What would be the process a user goes through if they collect additional data and wants to add them to the existing dataset? How automatic will this be?</p> <ul> <li>It is possible to re-run LSLAutoBIDS which would capture additional data from new subjects. Generally the idea is to run LSLAutoBIDS after each subject, then if there is an accidental overwrite we can still recover it due to versioning.</li> </ul> <p>2. What Datalad commands are used currently in the workflow? - We use <code>datalad save</code> to add and version the current state of the dataset and <code>datalad push</code>  to push the current state to the remote repository. We do not use the <code>datalad run</code> and <code>datalad rerun</code> capabilities as of now in our tool. Additionally, users can later use <code>datalad clone</code> to clone the repository and <code>datalad get</code> to get the actual data files (as they are stored in git-annex). - We use <code>datalad save</code> to add and version the current state of the dataset and <code>datalad push</code>  to push the current state to the remote repository. We do not use the <code>datalad run</code> and <code>datalad rerun</code> capabilities as of now in our tool. Additionally, users can later use <code>datalad clone</code> to clone the repository and <code>datalad get</code> to get the actual data files (as they are stored in git-annex).</p> <p>3. How automated is addition / deletion of a sample (e.g. new subject)? - Right now, adding a new sample requires calling the <code>lslautobids run</code> command, which could be run silently as well (e.g. via a regular cronjob). Deleting a sample/subject is not currently supported by the tool, but could be performed via Datalad. This is by design.  - Right now, adding a new sample requires calling the <code>lslautobids run</code> command, which could be run silently as well (e.g. via a regular cronjob). Deleting a sample/subject is not currently supported by the tool, but could be performed via Datalad. This is by design. </p> <p>4. Do you generate a separate DOI for every dataset version? - No, we have the same DOI current for the entire dataset, for all versions. Before publishing, we version the dataset via datalad using the same DOI, as Dataverse only supports versioning upon making the dataset public. </p> <p>5. Who controls the data upload process? - There is a user prompt asking the experimenter if they want to upload the subject recording immediately when we run the <code>lslautobids run</code>  command. We can also use the <code>--yes</code> flag of the <code>lslautobids run</code> command to force yes user input for all the user prompts throughout the run. - There is a user prompt asking the experimenter if they want to upload the subject recording immediately when we run the <code>lslautobids run</code>  command. We can also use the <code>--yes</code> flag of the <code>lslautobids run</code> command to force yes user input for all the user prompts throughout the run.</p> <p>6. Can you upload a subset of files ? - Yes, we have configurations in the project_config.toml file where the experimenter can specify to exclude certain subjects, certain tasks, and only exclude private project_other files.</p> <p>7. Can you upload to any other portals apart from Dataverse?  - It is not yet implemented as a choice but rather hard coded, but as long as a dataverse sibling is supported, many portals could be used (dataverse, openneuro, aws,...). Currently, on Dataverse as a sibling is supported by our tool.</p> <p>8. How do you handle data licensing? - Data license depends on the repository and can typically be chosen by the user typically upon making the dataset publicly available (or a data user agreement form can be employed). That being said, at OpenNeuro data is typically licensed CC0. </p> <p>9. How does LSLAutoBIDS incorporate addition / deletion of a sample (e.g. new subject)?</p> <ul> <li>Right now, adding a new sample requires calling the <code>lslautobids run</code> command, which could be run silently as well (e.g., via a regular cronjob). Deleting a sample/subject is not supported by the tool, but could be performed via <code>Datalad</code>. This is by design. </li> </ul> <p>10. What would be the process a user goes through if they collect additional data and want to add them to the existing dataset?</p> <ul> <li>It is possible to re-run LSLAutoBIDS which would capture additional data from new subjects. Generally the idea is to run LSLAutoBIDS after each subject, then if there is an accidental overwrite we can still recover it due to versioning.</li> </ul> <p>9. Troubleshooting: Datalad push to Dataverse command failed. - You might encounter errors such as: 'GitRepo' object has no attribute 'call_annex' and 'Datalad push command failed', this is because <code>git-annex</code> is required but not a Python package, and it needs to be installed sepearatly, run: <code>datalad-installer git-annex</code> after installing requirements.</p>"},{"location":"file_processing/","title":"File Processing","text":""},{"location":"file_processing/#file-processing-pipeline-processing_new_filespy","title":"File Processing Pipeline (<code>processing_new_files.py</code>)","text":"<p>The file processing part of the pipeline handles finding and processing new XDF files in the specified project directory:</p> <p>The pipeline ensures that all newly added data files are:</p> <ol> <li> <p>Detected since the last run.</p> </li> <li> <p>Filtered based on ignored subjects and tasks (these are specified in the project configuration file).</p> </li> <li> <p>Validated against duplicate or malformed filenames.</p> </li> <li> <p>Registered in the project configuration file (e.g., tasks ,etc.).</p> </li> <li> <p>Converted to BIDS format and uploaded, based on user confirmation or auto-run flags.</p> </li> </ol>"},{"location":"file_processing/#1-detection-of-new-files-check_for_new_data","title":"1. Detection of New Files (<code>check_for_new_data()</code>)","text":"<ul> <li> <p>Entry point of the script (from <code>main.py</code>).</p> </li> <li> <p>Reads project configuration (_config.toml) to identify ignored subjects and excluded tasks (which we don't want to process and include in the BIDS dataset). <li> <p>Calls <code>check_for_new_files()</code> to scan the project directory for files modified after the last recorded run.</p> </li>"},{"location":"file_processing/#2-filtering-check_for_new_files","title":"2. Filtering (<code>check_for_new_files()</code>)","text":"<ul> <li> <p>Uses <code>last_run_log.txt</code> to determine which files are new since the last check.</p> </li> <li> <p>Excludes:</p> <ul> <li> <p>Ignored subjects (defined in the project TOML config).</p> </li> <li> <p>Ignored tasks (tasks explicitly excluded in the project TOML config).</p> </li> </ul> </li> <li> <p>Returns a list of candidate <code>.xdf</code> files to process or outputs a message if no new files are found and exits.</p> </li> </ul>"},{"location":"file_processing/#3-validation-and-preparation-process_new_files","title":"3. Validation and Preparation \u2013 (<code>process_new_files()</code>)","text":"<p>For each new file:</p> <ul> <li> <p>File type check: Only .xdf files are considered.</p> </li> <li> <p>Duplicate prevention: Files with a _old suffix (e.g., sub-001_ses-01_task-Default_old.xdf) are flagged as duplicates. The pipeline logs an error and halts execution to avoid accidental overwrites. This is currently a specific duplicate check for LSL recordings. We shall extend this in future versions.</p> </li> <li> <p>Task extraction: Task names are parsed from filenames (e.g., sub-888_ses-001_task-Default_run-001_eeg.xdf \u2192 task = Default).</p> </li> <li> <p>Configuration update: Newly discovered tasks are appended to the TOML config under [Tasks.tasks].</p> </li> </ul>"},{"location":"file_processing/#4-user-prompt-conversion","title":"4. User Prompt &amp; Conversion","text":"<ul> <li>If cli_args.yes is set (as a flag while running <code>lslautobids run</code>), the pipeline skips user interaction and proceeds directly to conversion. In general, this flag sets all the user prompts throughout the pipeline to 'yes', allowing for fully automated runs. </li> <li> <p>Otherwise, the user is prompted to confirm whether BIDS conversion should start.</p> </li> <li> <p>If declined, the process halts with a warning.</p> </li> </ul>"},{"location":"file_processing/#5-bids-conversion-and-upload-bids_process_and_upload","title":"5. BIDS Conversion and Upload (<code>bids_process_and_upload()</code>)","text":"<ul> <li> <p>Invoked only if the user confirms (or auto-run flag is set).</p> </li> <li> <p>Handles BIDS formatting and uploading of the processed files.</p> </li> </ul>"},{"location":"file_processing/#6-supporting-utility","title":"6. Supporting Utility","text":"<ul> <li><code>_clear_last_run_log()</code> :Clears the last run timestamp (last_run_log.txt) when --redo_bids_conversion is specified (as a flag in <code>lslautobids run</code>), forcing reprocessing of all files.</li> </ul>"},{"location":"other_utils/","title":"Other Utility Functions","text":""},{"location":"other_utils/#other-utility-modules","title":"Other Utility Modules","text":""},{"location":"other_utils/#1-global-configuration-management-config_globalspy","title":"1. Global Configuration Management (<code>config_globals.py</code>)","text":"<p>This module manages global configuration settings and command-line arguments using a singleton pattern. The <code>CLIArgs</code> class ensures that there is only one instance of the configuration throughout the application. It provides methods to parse and retrieve command-line arguments and global configuration settings (lslautobids_config.yaml), which are then used across various modules.</p>"},{"location":"other_utils/#2-logging-configuration-config_loggerpy","title":"2. Logging Configuration (<code>config_logger.py</code>)","text":"<p>This module sets up a global logger for the application. The <code>get_logger()</code> function creates and configures a logger instance with a specified project name. It ensures that all log messages are formatted consistently and that log levels are set appropriately. The final log file is stored in the <code>&lt;BIDS_ROOT&gt;/&lt;project-name&gt;/code/</code> folder of the BIDS dataset.</p> <p>The log file is also available as a part of the created dataset.</p>"},{"location":"other_utils/#3-utility-functions-utilspy","title":"3. Utility Functions (<code>utils.py</code>)","text":"<p>This module contains various utility functions used across the application.  1. <code>get_user_input</code> : Handles user prompts and input validation. This function allows five attempts for valid input before exiting. The function takes the user prompt message as an argument and returns the user input. 2. <code>read_toml_file</code> : Reads and parses a TOML file, returning its contents as a dictionary. 3. <code>write_toml_file</code> : Writes a dictionary to a TOML file.</p>"},{"location":"testing/","title":"Tests","text":""},{"location":"testing/#testing","title":"Testing","text":"<p>The testing framework uses <code>pytest</code> to validate the functionality of the core components.</p> <ul> <li> <p>The tests are located in the <code>tests/</code> directory and cover various modules including configuration generation, file processing, BIDS conversion, DataLad integration, and Dataverse interaction. (Work in progress)</p> </li> <li> <p>The test directory contains :</p> <ul> <li><code>test_utils</code> : Directory containing utility functions needed across multiple test files.</li> <li><code>testcases</code> : Directory containing all the tests in a in a directory structure - <code>test_&lt;test_name&gt;</code>.</li> <li>Each <code>test_&lt;test_name&gt;</code> directory contains a <code>data</code> folder with sample data for that test and a <code>test_&lt;test_name&gt;.py</code> file with the actual test cases.</li> <li><code>run_all_tests.py</code> : A script to run all the tests in the <code>testcases</code> directory sequentially.</li> </ul> </li> </ul> <p>Tests will be added continuously as new features are added and existing features are updated.</p>"},{"location":"testing/#running-tests","title":"Running Tests","text":"<p>Before running the tests with <code>uv</code>, install the project in editable mode using <code>uv pip install --editable ./</code>. Then run the tests with <code>uv run pytest</code> (caveat, sometimes running all tests at once fails; in that case, you can run only the main functionality tests via <code>uv run pytest tests/test_main_functionality</code>).</p> <p>These tests ensure that each component functions as expected and that the overall pipeline works seamlessly. This tests will also be triggered automatically on each push or PR to the main repository using GitHub Actions.</p>"},{"location":"tutorial/","title":"Tutorial","text":""},{"location":"tutorial/#getting-started-with-lslautobids","title":"Getting Started with LSLAutoBIDS","text":"<p>This tutorial will guide you through the steps to set up and use the LSLAutoBIDS package for converting EEG recordings to BIDS format, version controlling the data with Datalad, and uploading it to a Dataverse repository with a practical example.</p>"},{"location":"tutorial/#installation-and-download","title":"Installation and Download","text":"<ol> <li>Clone the GitHub Repository <pre><code>git clone https://github.com/s-ccs/LSLAutoBIDS.git\n</code></pre></li> <li> <p>Pip install the package <pre><code>    cd LSLAutoBIDS\n    pip install .\n</code></pre> <code>git-annex</code> is required but not a Python package. You need to install it separately after installing the requirements, either with your system\u2019s package manager or simply by running: <pre><code>datalad-installer git-annex\n</code></pre></p> </li> <li> <p>Download the dummy dataset for testing in the LSLAutoBIDS root directory - (tutorial_sample_dataset)</p> </li> </ol> <p>The dataset has a sample project called \"test-project\" which contains an EEG recording file in the projects directory, a sample eyetracking recording in the <code>project_other/data</code> directory, and a dummy experimental code file in the <code>project_other/experiment</code> directory. <pre><code>sample_data\n\u2514\u2500\u2500 project_other\n  \u2514\u2500\u2500 test-project\n    \u251c\u2500\u2500 data\n      \u2514\u2500\u2500 sub-999\n        \u2514\u2500\u2500 ses-001\n          \u2514\u2500\u2500 beh\n            \u2514\u2500\u2500 behavioral_files((lab notebook, CSV, EDF file, etc))            \n    \u251c\u2500\u2500 experiment\n      \u2514\u2500\u2500 experimental_files (Matlab code, opensesame files, etc)\n\u2514\u2500\u2500 projects\n  \u2514\u2500\u2500 test-project\n    \u2514\u2500\u2500 sub-999\n      \u2514\u2500\u2500 ses-001\n        \u2514\u2500\u2500 eeg\n          \u2514\u2500\u2500 xdf file\n</code></pre></p>"},{"location":"tutorial/#configuration","title":"Configuration","text":"<ol> <li> <p>Generate the global configuration file <pre><code>lslautobids gen-dv-config\n</code></pre> This will create a configuration file template in folder <code>~/.config/lslautobids/</code>. This will create a config file with the dataverse details and the root directories for the projects.</p> </li> <li> <p>Create a Dataverse account and get the API token</p> </li> <li>Create a Dataverse account in your institution's Dataverse server (e.g., https://darus.uni-stuttgart.de/dataverse/darus)</li> <li>Create a new dataverse for your project</li> <li> <p>Create a new API token from your Dataverse account settings page (http://guides.dataverse.org/en/latest/user/index.html).</p> </li> <li> <p>Open the configuration file <code>~/.config/lslautobids/autobids_config.yaml</code> and fill in the details</p> </li> <li>Edit the file, e.g., via <code>nano ~/.config/lslautobids/autobids_config.yaml</code> to add the dataverse and project root details.</li> </ol> <p>Configuration file template: <pre><code>    \"BIDS_ROOT\": \"# relative to home/users directory: LSLAutoBIDS/sample_data/bids/\",       \n    \"PROJECT_ROOT\" : \"# relative to home/users: LSLAutoBIDS/sample_data/projects/\", \n    \"PROJECT_OTHER_ROOT\" : \"# path relative to home/users: LSLAutoBIDS/sample_data/project_other/\", \n    \"BASE_URL\": \"https://darus.uni-stuttgart.de\",  # The base URL for the service.\n    \"API_KEY\": \"# Paste your dataverse API token here\", # Your API token for authentication.\n    \"PARENT_DATAVERSE_NAME\": \"simtech_pn7_computational_cognitive_science\" # The name of the dataverse to which datasets will be uploaded. When you in the dataverses page , you can see this name in the URL after 'dataverse/'.\n</code></pre> This will be mostly same for all the projects, thus running this command is only recommended once per system.</p> <ol> <li>Create a project-specific configuration file This will create a project-specific configuration file template in the specified project root directory.</li> </ol> <pre><code>lslautobids gen-proj-config --project test-project\n</code></pre> <p>This will create a test-project_config.toml file in the project root directory. </p> <p>[!NOTE]: For the rest of the tutorial, we are assuming that we place the downloaded sample_data in the root of the cloned LSLAutoBIDS repository and <code>LSLAutoBIDS</code> is cloned in the <code>home/users/</code> folder. In this case, the projects root will be <code>LSLAutoBIDS/sample_data/projects/</code> and so on for project_other and bids.</p> <p>Fill in the details in the configuration file <code>LSLAutoBIDS/sample_data/projects/test-project/test-project_config.toml</code> file.</p> <p>You can find the details about the parameters in the comments of the template configuration file generated. For this tutorial, you might want to just change the author and email fields. Rest of the fields are already filled in for the test project.</p>"},{"location":"tutorial/#example-case-1","title":"Example Case 1","text":"<p>A lab wants to conduct an EEG-EyeTracking experiment and wants to make this dataset publicly available for the other neuroscience researchers. To assure data provenance and reproducibility within and across labs, they want to have a standardized structure for storing the data and code files. </p> <p>In this example, we will see how to use the LSLAutoBIDS package to: 1. Convert the recorded EEG data in <code>xdf</code> format to BIDS format. 2. Integrate other data files (e.g. eye-tracking recording, experiment code files) into the dataset (Note: LSLAutoBIDS does not do any conversion of these files into BIDS format, it just copies these files to the appropriate directories in the BIDS dataset in a psuedo-BIDS like structure). 3. Version control the data and code files using Datalad. 4. Upload the dataset to a Dataverse repository for public access.</p>"},{"location":"tutorial/#how-to-run-the-example","title":"How to run the example?","text":"<ol> <li>Check if the toml configuration file <code>LSLAutoBIDS/data/projects/test-project/test-project_config.toml</code> is filled in with the correct details, specially the <code>otherFilesUsed</code> and <code>expectedOtherFiles</code> fields. For this example we are using eye tracking data as a behavioral file, thus the otherFilesUsed field should be set to true and the <code>expectedOtherFiles</code> field should contain the expected other files (non-eeg files) extensions.</li> <li>Check if the toml configuration file <code>LSLAutoBIDS/sample_data/projects/test-project/test-project_config.toml</code> is filled in with the correct details, specially the OtherFilesUsed and expectedOtherFiles fields. For this example, we are using eye tracking data as a behavioral file, thus the otherFilesUsed field should be set to true and the expectedFiles field should contain the expected other file extensions. <pre><code>  [OtherFilesInfo]\n    otherFilesUsed = true\n\n  [OtherFilesInfo]\n    expectedOtherFiles = [\"*.edf\"=\"misc/{prefix}_et.edf\", \"*.csv\"=\"misc/{prefix}_beh.csv\", \"*_labnotebook.tsv\"=\"misc/{prefix}_labnotebook.tsv\", \"*_participantform.tsv\"=\"{prefix}_participantform.tsv\"]\n</code></pre></li> <li> <p>Run the conversion and upload command to convert the <code>xdf</code> files to BIDS format and upload the data to the dataverse. <pre><code>lslautobids run -p test-project\n</code></pre></p> </li> <li> <p>This will convert the xdf file in the <code>LSLAutoBIDS/sample_data/projects/test-project/sub-999/ses-001/eeg/</code> directory to BIDS format and store it in the <code>LSLAutoBIDS/sample_data/bids/test-project/sub-999/ses-001/</code> directory. </p> </li> <li>You can check the logs in the log file <code>LSLAutoBIDS/sample_data/bids/test-project/code/test-project.log</code> file. </li> <li>The source data i.e., the raw <code>xdf</code> file, behavioral data (e.g. eye-tracking recording) and the experimental code files in <code>PROJECT_OTHER_ROOT/test-project/experiment</code> (all files e.g., <code>.py</code>, <code>.oxexp</code> will be compressed to a <code>tar.gz</code> archive) will be copied to the <code>LSLAutoBIDS/sample_data/bids/test-project/source_data/</code>, <code>LSLAutoBIDS/sample_data/bids/test-project/beh/</code> and <code>LSLAutoBIDS/sample_data/bids/test-project/misc/</code> directories respectively.</li> </ol>"},{"location":"tutorial/#example-case-2","title":"Example Case 2","text":"<p>In this case, the experimenter wants to publish only the raw EEG recordings and the converted EEG files, but wants to exclude the other files and experiment code.</p>"},{"location":"tutorial/#how-to-run-the-example_1","title":"How to run the example?","text":"<ol> <li>The workflow is almost identical to Example Case 1, except other and experiment files are excluded.</li> <li>Check if the toml configuration file <code>LSLAutoBIDS/sample_data/projects/test-project/test-project_config.toml</code> is filled in with the correct details.</li> </ol> <p><pre><code>  [otherFilesInfo]\n    expectedOtherFiles = False\n</code></pre> 3. Run the conversion and upload command to convert the <code>xdf</code> files to BIDS format and upload the data to the dataverse. <pre><code>lslautobids run -p test-project\n</code></pre>   1. This will convert the <code>xdf</code> file, and you can check the logs in the log file as shown in example case 1.   2. <code>LSLAutoBIDS/sample_data/bids/test-project/beh</code> and <code>LSLAutoBIDS/sample_data/bids/test-project/misc</code> directories do not exist in the converted bids. </p>"},{"location":"tutorial/#after-publishing-the-dataset-out-of-scope-of-this-package","title":"After publishing the dataset (Out of Scope of this package)","text":"<p>Once the dataset is published in Dataverse, other researchers can access the dataset and also cite the dataset using the DOI provided by that Dataverse dataset.</p> <p>You can clone the dataset using Datalad and access the data files.</p> <pre><code>datalad clone &lt;dataverse-dataset-url&gt;\n</code></pre> <p>Since the dataset is version controlled using datalad, the large files are not downloaded by default as they are stored in a git-annex. You can get the files using the datalad get command.</p> <pre><code>datalad get &lt;file-path&gt;\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#how-to-run-the-software","title":"How to run the software?","text":""},{"location":"usage/#step-1-clone-the-repository","title":"Step 1: Clone the repository","text":"<pre><code>git clone https://github.com/s-ccs/LSLAutoBIDS.git\n</code></pre>"},{"location":"usage/#step-2-install-the-package","title":"Step 2: Install the package","text":"<p>Go to the cloned directory and install the package using pip. <pre><code>cd LSLAutoBIDS\npip3 install lslautobids\n</code></pre> It is advised to install the package in a separate environment (e.g. using <code>conda</code> or <code>virtualenv</code>).</p> <p>Note</p> <p>If you are using conda, you can create a new conda environment using the following command and activate it.</p> <pre><code>conda create -n &lt;ENV_NAME&gt; python=3.11\nconda activate &lt;ENV_NAME&gt;\n</code></pre>"},{"location":"usage/#step-3-data-organization","title":"Step 3: Data Organization","text":"<p>The package requires the recorded XDF data to be organized in a specific directory structure following the BIDS (Brain Imaging Data Structure) naming convention. This is the default output of the popular labrecorder. The BIDS format is a standard for organizing and describing neuroimaging data, making it easier to share and analyze.</p> <ul> <li>The <code>projects</code> root location is the root directory where all the eeg raw recordings (say <code>.xdf</code> files) are stored e.g. <code>projects/sub-A/ses-001/eeg/sub-A_ses-001_task-foo.xdf</code>.</li> <li>The (optional) <code>project_other</code> root location is the directory where the experiments (e.g <code>.py</code>, <code>.oxexp</code>) and behavioral files (e.g. eye-tracking recordings, labnotebook, participant forms, etc ) are stored.</li> <li>The <code>bids</code> root location is the directory where the converted BIDS data is stored, along with source data and code files which we want to version control using <code>Datalad</code>.</li> </ul> <p>Important</p> <p>The package is looking in these paths, and extracts some metadata from the BIDS-like names. You can change the location of the root directories according to your preference. You must also strictly follow the naming convention for the project and subject subdirectories.</p> <p>Here  you will find the recommended directory structure for storing the project data (recorded, other and converted data) in the data_organization file.</p>"},{"location":"usage/#step-4-generate-the-configuration-files","title":"Step 4: Generate the configuration files","text":"<p>This configuration is required to run for running the automation pipeline of <code>lslautobids</code>. </p> <ol> <li>AutoBIDS and Dataverse Configuration : </li> <li>Run the command below to create a configuration file template in folder <code>~/.config/lslautobids/</code> folder. This will create a config file with the dataverse details and the root directories for the projects.         </li> </ol> <p><pre><code>lslautobids gen-dv-config\n</code></pre> - Edit the file e.g. via <code>nano ~/.config/lslautobids/autobids_config.yaml</code> to add the dataverse and project root details.</p> <p>This will be mostly same for all the projects, thus running this command is only recommended once per system.</p> <ol> <li>Project Configuration : This is to be done once for each new project. This store the project details like project name, project id, project description etc. and is saved in a <code>projects/&lt;PROJECT_NAME&gt;/project_&lt;PROJECT_NAME&gt;.toml</code> file </li> <li>Run the command below to create a configuration file template in <code>projects/&lt;PROJECT_NAME&gt;/</code> folder (according to the selected root directories).</li> </ol> <pre><code>lslautobids gen-proj-config -p &lt;projectname&gt; \n</code></pre>"},{"location":"usage/#step-6-run-the-conversion-scripts","title":"Step 6: Run the conversion scripts","text":"<p>Run the conversion scripts to convert the xdf files to BIDS format and upload the data to the dataverse. <pre><code>lslautobids run -p TestData2025\n</code></pre></p> <p>Note</p> <p>You can run the <code>--help</code> for all the commands to get more information about the available options and directly <code>lsl-autobids help</code> to get the list of available commands.</p>"}]}